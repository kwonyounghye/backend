{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "# 긍정리뷰 100개 불러오기\n",
    "pos_review=(glob.glob(\"c:/data/imdb/train/pos/*.txt\"))[:100]\n",
    "lines_pos=[]\n",
    "for i in pos_review:\n",
    "    try:\n",
    "        f = open(i, 'r')\n",
    "        temp = f.readlines()[0]\n",
    "        lines_pos.append(temp)\n",
    "        f.close()\n",
    "    except Exception as e:\n",
    "        continue\n",
    "len(lines_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2632)\t0.10430305595720811\n",
      "  (0, 1348)\t0.10430305595720811\n",
      "  (0, 1318)\t0.07242968766333299\n",
      "  (0, 3580)\t0.05164463226548277\n",
      "  (0, 146)\t0.08961436013929301\n",
      "  (0, 129)\t0.09571071971922811\n",
      "  (0, 2176)\t0.04914865560743787\n",
      "  (0, 1250)\t0.07242968766333299\n",
      "  (0, 3892)\t0.09571071971922811\n",
      "  (0, 2470)\t0.03485607054883035\n",
      "  (0, 3023)\t0.10430305595720811\n",
      "  (0, 1859)\t0.10430305595720811\n",
      "  (0, 2070)\t0.081022023901313\n",
      "  (0, 630)\t0.0749256643213779\n",
      "  (0, 2855)\t0.10430305595720811\n",
      "  (0, 1798)\t0.09571071971922811\n",
      "  (0, 491)\t0.10430305595720811\n",
      "  (0, 3697)\t0.09571071971922811\n",
      "  (0, 2916)\t0.10430305595720811\n",
      "  (0, 3404)\t0.20860611191441622\n",
      "  (0, 1179)\t0.10430305595720811\n",
      "  (0, 3050)\t0.06633332808339788\n",
      "  (0, 1974)\t0.08488565627825163\n",
      "  (0, 3067)\t0.10430305595720811\n",
      "  (0, 2907)\t0.08961436013929301\n",
      "  :\t:\n",
      "  (99, 1726)\t0.08226721859378414\n",
      "  (99, 1226)\t0.06764092294931459\n",
      "  (99, 2374)\t0.08896092696102158\n",
      "  (99, 1315)\t0.08537419078875624\n",
      "  (99, 194)\t0.09320313545124304\n",
      "  (99, 1532)\t0.08537419078875624\n",
      "  (99, 3397)\t0.08226721859378414\n",
      "  (99, 1201)\t0.050750017818456665\n",
      "  (99, 1)\t0.23078108658744342\n",
      "  (99, 3872)\t0.062140693031738525\n",
      "  (99, 3583)\t0.09320313545124304\n",
      "  (99, 2353)\t0.07097055453279418\n",
      "  (99, 3366)\t0.19019614337727941\n",
      "  (99, 366)\t0.12428138606347705\n",
      "  (99, 2335)\t0.03281745344117876\n",
      "  (99, 3730)\t0.059811978286827806\n",
      "  (99, 3108)\t0.05670500609185569\n",
      "  (99, 3177)\t0.22457257087801621\n",
      "  (99, 1568)\t0.20773869106251994\n",
      "  (99, 2161)\t0.08537419078875624\n",
      "  (99, 1544)\t0.07952667489367608\n",
      "  (99, 3579)\t0.07283296652643863\n",
      "  (99, 2575)\t0.10623653983918074\n",
      "  (99, 1561)\t0.07283296652643863\n",
      "  (99, 3099)\t0.044244713140832\n",
      "(100, 4001)\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.06538462 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.23078109 0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tokenizer = RegexpTokenizer('[\\w]+')\n",
    "stop_words = stopwords.words('english')\n",
    "# TF-IDF 가중치 할당\n",
    "vec = TfidfVectorizer(stop_words=stop_words)\n",
    "vector_lines_pos = vec.fit_transform(lines_pos)\n",
    "print(vector_lines_pos)\n",
    "A=vector_lines_pos.toarray()\n",
    "print(A.shape)\n",
    "print(A)\n",
    "# x축 단어, y축 문서\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4001, 100)\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.06538462 0.23078109]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# 현재 상태는 100개의 문서의 유사도\n",
    "# 단어간의 유사도를 구하는 것이 목적이므로\n",
    "# 단어-문서 행렬로 변경\n",
    "# x축 문서, y축 단어\n",
    "A=A.transpose()\n",
    "print(A.shape)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.5\n",
      "  (1, 1)\t1.0\n",
      "  (2, 0)\t0.7\n",
      "  (2, 2)\t1.5\n",
      "[[0.5 0.  0. ]\n",
      " [0.  1.  0. ]\n",
      " [0.7 0.  1.5]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse\n",
    "# 밀집행렬 (dense array): 0을 포함하여 모든 값이 다 채워진 행렬\n",
    "a=np.array([[0.5,0,0],[0,1,0],[0.7,0,1.5]])\n",
    "# (sparse array) 밀집행렬을 희소행렬로 변환\n",
    "# 밀집행렬의 단점 : 0이 많을 경우 메모리 낭비가 될 수 있음\n",
    "# 희소행렬은 0이 아닌 값들의 위치와 값만 기록하여 메모리를 절약하는 방식\n",
    "b=sparse.csr_matrix(a) # 희소행렬: 0이 아닌 값만 채워진 행렬(메모리 낭비x)\n",
    "print('{}'.format(b))\n",
    "# (0,0) 0.5 => 인덱스 0,0에 값 0.5\n",
    "# (1,1) 1 => 인덱스 1,1에 값 1\n",
    "# (2,0) 0.7 => 인덱스 2,0에 값 0.7\n",
    "# (2,2) 1 => 인덱스 2,2에 값 1.5\n",
    "c=b.toarray() # 희소행렬을 밀집행렬로 변환\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1469, 108), 0.37803585968894865),\n",
       " ((1470, 108), 0.2189685434746738),\n",
       " ((1476, 108), 0.06407477897013734),\n",
       " ((1477, 108), 0.185189577514238),\n",
       " ((1480, 108), 0.20111036876169444),\n",
       " ((1489, 108), 0.06995711757772019),\n",
       " ((1496, 108), 0.10714874067068783),\n",
       " ((1503, 108), 0.30487333830091773),\n",
       " ((1504, 108), 0.30487333830091773),\n",
       " ((1512, 108), 0.30487333830091773)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "A_sparse = sparse.csr_matrix(A)\n",
    "# 코사인 유사도(같은 방향인지 반대방향인지 확인) 계산(A_sparse 필요)\n",
    "similarities_sparse = cosine_similarity(A_sparse,dense_output=False)\n",
    "# todok() 행렬을 딕셔너리 형태로 변환\n",
    "list(similarities_sparse.todok().items())[35000:35010]\n",
    "# list(similarities_sparse.todok().items())[-10:]\n",
    "#단어 이름이 아닌 인덱스 형태로 출력됨\n",
    "# (1469,108), 0.37 1469 단어와 108 단어의 유사도는 37%\n",
    "# (3968, 3988 단어의 코사인 유사도는 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'fraud'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.get_feature_names()[1469]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'actual'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.get_feature_names()[108]\n",
    "#fraud actual의 유사도는 37%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(3971, 3372)</td>\n",
       "      <td>0.499961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(3372, 3971)</td>\n",
       "      <td>0.499961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(1192, 2554)</td>\n",
       "      <td>0.499958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(2554, 1192)</td>\n",
       "      <td>0.499958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(2468, 1321)</td>\n",
       "      <td>0.499957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(2468, 710)</td>\n",
       "      <td>0.499957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(710, 2468)</td>\n",
       "      <td>0.499957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(1321, 2468)</td>\n",
       "      <td>0.499957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(2146, 889)</td>\n",
       "      <td>0.499909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(889, 2146)</td>\n",
       "      <td>0.499909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          words    weight\n",
       "0  (3971, 3372)  0.499961\n",
       "1  (3372, 3971)  0.499961\n",
       "2  (1192, 2554)  0.499958\n",
       "3  (2554, 1192)  0.499958\n",
       "4  (2468, 1321)  0.499957\n",
       "5   (2468, 710)  0.499957\n",
       "6   (710, 2468)  0.499957\n",
       "7  (1321, 2468)  0.499957\n",
       "8   (2146, 889)  0.499909\n",
       "9   (889, 2146)  0.499909"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# 위의 결과값을 데이터프레임으로 출력\n",
    "df=pd.DataFrame(list(similarities_sparse.todok().items()),columns=[\"words\",\"weight\"])\n",
    "df2=df.sort_values(by=['weight'],ascending=False)\n",
    "df2=df2.reset_index(drop=True)\n",
    "# 단어 자신끼리의 짝은 1이 되므로 1보다 작은 항목들만 출력시킴\n",
    "df3=df2.loc[np.round(df2['weight']) < 1]\n",
    "df3=df3.reset_index(drop=True)\n",
    "df3.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff4f85d6e04298634172ac5d8264e7e9b556b95639fe52ebb9425c4d4cba0c9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
