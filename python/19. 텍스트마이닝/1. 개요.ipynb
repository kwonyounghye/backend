{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n",
      "HELLO WORLD\n"
     ]
    }
   ],
   "source": [
    "s=\"Hello World\"\n",
    "print(s.lower()) #소문자로 변환\n",
    "print(s.upper()) #대문자로 변환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 올해 들어 서울 지역의 부동산 가격이 % 하락했습니다\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# 숫자 제거\n",
    "p=re.compile(\"[0-9]+\") # 숫자 + 반복\n",
    "# 지정된 패턴에 맞는 부분이 제거됨\n",
    "result=p.sub(\"\",\" 올해 들어 서울 지역의 부동산 가격이 30% 하락했습니다\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 올해 들어 서울 지역의 부동산 가격이 30% 하락했습니다!#$_$1234\n",
      " 올해 들어 서울 지역의 부동산 가격이  하락했습니다\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def clean_text(input_data):\n",
    "# 텍스트에 포함되어 있는 숫자와 특수문자 제거\n",
    "    p=re.compile(\"[0-9_!@#$%^&*+.]\")\n",
    "    result=p.sub(\"\",input_data)\n",
    "    return result\n",
    "txt = \" 올해 들어 서울 지역의 부동산 가격이 30% 하락했습니다!#$_$1234\"\n",
    "print(txt)\n",
    "print(clean_text(txt))\n",
    "# 숫자, 특수문자, 문장부호 등은 큰 의미가 없으므로 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['추석', '연휴', '민족', '대이동', '시작', '교통량', '교통사고', '특히', '자동차', '고장', '상당수']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=['추석', '연휴', '민족', '대이동', '시작', '늘어', '교통량', '교통사고', '특히', '자동차', '고장', '상당수', '나타', '것', '기자']\n",
    "# 불용어\n",
    "stopwords=['가다', '늘어', '나타', '것', '기자']\n",
    "# 불용어 제거\n",
    "[i for i in words if i not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tjoeun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk # NLTK 패키지\n",
    "nltk.download(\"stopwords\") # 불용어 사전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chief', 'justice', 'roberts', ',', 'president', 'carter', ',', 'president', 'clinton', 'president', 'bush', 'obama', 'fellow', 'americans', 'people', 'world', 'thank']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "words=[\"chief\",\"justice\",\"roberts\",\",\",\"president\",\"carter\",\",\",\"president\",\"clinton\",\"president\",\"bush\",\"obama\",\"fellow\",\"americans\",\"and\",\"people\",\"of\",\"the\",\"world\",\"thank\",\"you\"]\n",
    "print([w for w in words if not w in stopwords.words(\"english\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tjoeun\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\") # 문장 tokenizer 다운로드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook cooker cook cook cookeri "
     ]
    }
   ],
   "source": [
    "# 어근 동일화 처리: 비슷한 어근 처리(stemming)\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stm = PorterStemmer()\n",
    "txt=\"cook cooker cooking cooks cookery\"\n",
    "words=word_tokenize(txt)\n",
    "for w in words:\n",
    "    print(stm.stem(w),end=\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python python python python python "
     ]
    }
   ],
   "source": [
    "stm=PorterStemmer()\n",
    "#어근이 동일한 키워드 정리\n",
    "txt=\"pythoning pythons Python pythoners pythoned\"\n",
    "words=word_tokenize(txt)\n",
    "for w in words:\n",
    "    print(stm.stem(w),end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook cook cook cook cookery "
     ]
    }
   ],
   "source": [
    "#LancasterStemmer : PorterStemmer와 비슷하지만 좀더 나은 성능\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stm=LancasterStemmer()\n",
    "txt=\"cook cooker cooking cooks cookery\"\n",
    "words=word_tokenize(txt)\n",
    "for w in words:\n",
    "    print(stm.stem(w),end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python python python python python "
     ]
    }
   ],
   "source": [
    "stm=LancasterStemmer()\n",
    "txt=\"pythoning pythons Python pythoners pythoned\"\n",
    "words=word_tokenize(txt)\n",
    "for w in words:\n",
    "    print(stm.stem(w),end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook\n",
      "cookery\n",
      "leside\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.regexp import RegexpStemmer\n",
    "stm = RegexpStemmer('ing')\n",
    "print(stm.stem('cooking'))\n",
    "print(stm.stem('cookery'))\n",
    "print(stm.stem('ingleside'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ing s Python ers ed "
     ]
    }
   ],
   "source": [
    "stm=RegexpStemmer(\"python\")\n",
    "txt=\"pythoning pythons Python pythoners pythoned\"\n",
    "words=word_tokenize(txt)\n",
    "for w in words:\n",
    "    print(stm.stem(w),end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He\n",
      "el\n",
      "ll\n",
      "lo\n"
     ]
    }
   ],
   "source": [
    "txt = 'Hello'\n",
    "# 2-gram이므로 문자열의 끝에서 한 글자 앞까지만 반복함\n",
    "for i in range(len(txt) - 1):\n",
    "    # 현재 문자와 그다음 문자 출력\n",
    "    print(txt[i], txt[i + 1], sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is\n",
      "is python\n",
      "python script\n"
     ]
    }
   ],
   "source": [
    "txt = 'this is python script'\n",
    "# 공백을 기준으로 문자열을 분리하여 리스트로 저장\n",
    "words = txt.split()\n",
    "# 2-gram이므로 리스트의 마지막에서 요소 한 개 앞까지만 반복함\n",
    "for i in range(len(words) - 1):\n",
    "    # 현재 문자열과 그다음 문자열 출력\n",
    "    print(words[i], words[i + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he\n",
      "el\n",
      "ll\n",
      "lo\n"
     ]
    }
   ],
   "source": [
    "txt = 'hello'\n",
    "two_gram = zip(txt, txt[1:])\n",
    "for i in two_gram:\n",
    "    print(i[0], i[1], sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'is'), ('is', 'python'), ('python', 'script')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = 'this is python script'\n",
    "words = txt.split()\n",
    "list(zip(words, words[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'love') ('love', 'you.') ('you.', 'Good') ('Good', 'morning.') ('morning.', 'Good') ('Good', 'bye.') "
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "sentence=\"I love you. Good morning. Good bye.\"\n",
    "grams=ngrams(sentence.split(),2)\n",
    "for gram in grams:\n",
    "    print(gram,end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'love', 'you.') ('love', 'you.', 'Good') ('you.', 'Good', 'morning.') ('Good', 'morning.', 'Good') ('morning.', 'Good', 'bye.') "
     ]
    }
   ],
   "source": [
    "sentence=\"I love you. Good morning. Good bye.\"\n",
    "grams=ngrams(sentence.split(),3)\n",
    "for gram in grams:\n",
    "    print(gram,end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ff4f85d6e04298634172ac5d8264e7e9b556b95639fe52ebb9425c4d4cba0c9c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
